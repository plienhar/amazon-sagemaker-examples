{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872c9a95-4bed-40b5-a76a-2cf5982c1153",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Serve and stream tokens from LMSYS's `vicuna-7b-v1.3` hosted on Amazon SageMaker using LMI (Large Model Inference) DJL-based container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66ada4-d677-4900-a01c-0524090a1ce4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f4e2d-9d88-4038-9129-406edf604197",
   "metadata": {},
   "source": [
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "This notebook focuses on deploying the [`lmsys/vicuna-7b-v1.3`](https://huggingface.co/lmsys/vicuna-7b-v1.3) HuggingFace model to a SageMaker Endpoint for a text generation task. In this example, you will use the SageMaker-managed [LMI (Large Model Inference)](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-dlc.html) Docker image as inference image. LMI images features a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/).\n",
    "\n",
    "Once the model has been deployed, you will submit text generation requests and get a streamed response in return using SageMaker's native response streaming capability.\n",
    "\n",
    "In this notebook, we make an extensive use of the higher-level abstractions provided by the [`sagemaker` Python SDK](https://sagemaker.readthedocs.io/en/stable/index.html) to which we delegate the management of as many resources and configuration as we can, hence demonstrating that the deployment of LLMs to SageMaker can be performed with great simplicity and minimal amount of code.\n",
    "\n",
    "You will successively deploy the `lmsys/vicuna-7b-v1.3` model twice using the HuggingFace Accelerate engine on a `ml.g5.2xlarge` GPU instance (1 device with 24 GiB of device memory):\n",
    "* Once without writing any custom server-side Python handler script and therefore leveraging the fact that the default Python handlers of the LMI DLC natively support streaming for the HuggingFace Accelerate engine (among others),\n",
    "* Once with a custom server-side Python handler script. \n",
    "\n",
    "Notice that when using the default handlers, streaming cannot be disabled once the endpoint has been deployed with `streaming_enabled` set to `True`, i.e. the endpoint can only be invoked using `sagemaker::InvokeWithStreamingResponse` (and not `sagemaker::InvokeEndpoint`). On the other hand, when implementing a custom handler script, we will be able to choose between streaming our responses or not on a per-request basis.\n",
    "\n",
    "**Notices:**\n",
    "* Make sure that the `ml.g5.2xlarge` instance type is available in your AWS Region.\n",
    "* Make sure that the value of your \"ml.g5.2xlarge for endpoint usage\" Amazon SageMaker service quota allows you to deploy one Endpoint using this instance type.\n",
    "\n",
    "### License agreement\n",
    "* This model is not intended for commercial use, cf. model card for more information about licensing.\n",
    "* This notebook is a sample notebook and not intended for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa8856-71c9-4333-b734-bd28b0071e08",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Execution environment setup\n",
    "This notebook requires the following third-party Python dependencies:\n",
    "* AWS [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html#)\n",
    "* AWS [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/index.html). Since we use the 0.23.0 version of the DJL LMI DLC, the minimal SDK version is 2.173.0.\n",
    "* HuggingFace [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index)\n",
    "\n",
    "Let's install or upgrade these dependencies using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1edbb8-0f0a-4c5c-bcb3-9d3e2be5222e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pip --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d2481f-e1f6-495b-bc52-bb8347cad3ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: TO BE REMOVED\n",
    "!pip install dependencies/botocore-*-py3-none-any.whl dependencies/boto3-*-py3-none-any.whl --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee0c429-f178-47b4-af48-a04cf8bac760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: TO BE REMOVED \n",
    "import boto3\n",
    "assert boto3.__version__ == \"1.26.157\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f0d19-e197-43ff-9b6a-9fb31d84816a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.173.0\" huggingface_hub --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ddf083-868f-4775-b2cb-2ff34dd5f8ca",
   "metadata": {},
   "source": [
    "### Imports & global variables assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1697d3bb-040b-452e-a79e-9b17c717cca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import boto3\n",
    "import huggingface_hub\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9db6a2-2510-4844-b73f-fe050f52436b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SM_DEFAULT_EXECUTION_ROLE_ARN = sagemaker.get_execution_role()\n",
    "SM_SESSION = sagemaker.session.Session()\n",
    "SM_ARTIFACT_BUCKET_NAME = SM_SESSION.default_bucket()\n",
    "\n",
    "REGION_NAME = SM_SESSION._region_name\n",
    "S3_CLIENT = boto3.client(\"s3\", region_name=REGION_NAME)\n",
    "# TODO: TO BE MODIFIED: sagemaker-runtime-demo -> sagemaker-runtime\n",
    "SAGEMAKER_RUNTIME_CLIENT = boto3.client(\"sagemaker-runtime-demo\", region_name=REGION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b34d1c-cf7a-4f7e-8091-215c52c3aba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HOME_DIR = os.environ[\"HOME\"]\n",
    "\n",
    "# HuggingFace local model storage\n",
    "HF_LOCAL_CACHE_DIR = Path(HOME_DIR) / \".cache\" / \"huggingface\" / \"hub\"\n",
    "HF_LOCAL_DOWNLOAD_DIR = Path.cwd() / \"model_repo\"\n",
    "HF_LOCAL_DOWNLOAD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Inference code local storage\n",
    "SOURCE_DIR = Path.cwd() / \"code\"\n",
    "SOURCE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Selected HuggingFace model\n",
    "HF_HUB_MODEL_NAME = \"lmsys/vicuna-7b-v1.3\"\n",
    "PROMPT_TEMPLATE = \"USER: {prompt}\\nAssistant:\"\n",
    "\n",
    "# HuggingFace remote model storage (Amazon S3)\n",
    "HF_MODEL_KEY_PREFIX = f\"hf-large-model-djl/{HF_HUB_MODEL_NAME}\"\n",
    "\n",
    "# Other global constants\n",
    "DJL_VERSION = \"0.23.0\" # requires sagemaker>=2.173.0 \n",
    "INSTANCE_TYPE = \"ml.g5.2xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c7ba34-d9e0-4919-a6de-771f7d04d33b",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ad83d-62b5-4dfc-b800-cac6c61368ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_s3_objects(bucket: str, key_prefix: str) -> List[Dict[str, Any]]:\n",
    "    paginator = S3_CLIENT.get_paginator(\"list_objects\")\n",
    "    operation_parameters = {\"Bucket\": bucket, \"Prefix\": key_prefix}\n",
    "    page_iterator = paginator.paginate(**operation_parameters)\n",
    "    return [obj for page in page_iterator for obj in page[\"Contents\"]]\n",
    "\n",
    "\n",
    "def delete_s3_objects(bucket: str, keys: str) -> None:\n",
    "    S3_CLIENT.delete_objects(Bucket=bucket, Delete={\"Objects\": [{\"Key\": key} for key in keys]})\n",
    "\n",
    "\n",
    "def get_local_model_cache_dir(hf_model_name: str) -> str:\n",
    "    for dir_name in os.listdir(HF_LOCAL_CACHE_DIR):\n",
    "        if dir_name.endswith(hf_model_name.replace(\"/\", \"--\")):\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"Could not find HF local cache directory for model {hf_model_name}\")\n",
    "    return HF_LOCAL_CACHE_DIR / dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79063d2-192d-43fa-acad-9c275da66d5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StreamScanner:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the InvokeEndpointWithResponseStream event stream. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'readlines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.buff = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def write(self, content: bytes) -> None:\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "        \n",
    "    def readlines(self) -> bytes:\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            if line[-1] != b'\\n':\n",
    "                self.read_pos += len(line)\n",
    "                yield line[:-1]\n",
    "                \n",
    "    def reset(self) -> None:\n",
    "        self.read_pos = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66276d81-05b6-4c06-9570-1e1047a68930",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Model upload to Amazon S3\n",
    "Models served by a LMI container can be downloaded to the container in different ways:\n",
    "* Like all the SageMaker Inference containers, having the container to download the model from Amazon S3 as a single `model.tar.gz` file. In the case of LLMs, this approach is discouraged since downloading and decompression times can become unreasonably high.\n",
    "* Having the container to download the model directly from the HuggingFace Hub for you. This option may involve high download times too.\n",
    "* Having the container to download the uncompressed model from Amazon S3 with maximal throughput by using the [`s5cmd`](https://github.com/peak/s5cmd) utility. This option is specific to LMI containers and is the recommended one. It requires however, that the model has been previously uploaded to a S3 Bucket. \n",
    "\n",
    "In this section, you will:\n",
    "1. Download the model from the HuggingFace Hub to your local host,\n",
    "2. Upload the downloaded model to a S3 Bucket. This notebook uses the SageMaker's default regional Bucket. Feel free to upload the model to the Bucket of your choice by modifying the `SM_ARTIFACT_BUCKET_NAME` global variable accordingly.\n",
    "\n",
    "Each operation takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b12738-ce43-4a02-bbf8-a3c4e591a762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_hub.snapshot_download(\n",
    "    repo_id=HF_HUB_MODEL_NAME,\n",
    "    revision=\"main\",\n",
    "    local_dir=HF_LOCAL_DOWNLOAD_DIR,\n",
    "    local_dir_use_symlinks=\"auto\",  # Files larger than 5MB are actually symlinked to the local HF cache\n",
    "    allow_patterns=[\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\", \"*.py\"],\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da993988-8fa4-4b10-89c3-fe7688836822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = SM_SESSION.upload_data(\n",
    "    path=HF_LOCAL_DOWNLOAD_DIR.as_posix(),\n",
    "    bucket=SM_ARTIFACT_BUCKET_NAME,\n",
    "    key_prefix=HF_MODEL_KEY_PREFIX,\n",
    ")\n",
    "print(f\"Model artifacts have been successfully uploaded to: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c389f-c15a-4036-a511-4791db9ef9bf",
   "metadata": {},
   "source": [
    "The `huggingface_hub.snapshot_download` function downloaded the model repository to a cache located in your home directory. Downloaded files were duplicated in the target local download directory. Large files (larger than 5 MB) were not duplicated however but simply symlinked. Still, uncompressed LLM artifacts consume disk space. The two following cells removes the downloaded files from your local host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8be488-d83d-40af-9d04-de4f016ae667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove HF model artifacts from the local download directory\n",
    "shutil.rmtree(HF_LOCAL_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194217d0-5712-466c-992c-4e698f8f8f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove HF model artifacts from the local HF cache directory\n",
    "hf_local_cache_dir = get_local_model_cache_dir(hf_model_name=HF_HUB_MODEL_NAME)\n",
    "shutil.rmtree(hf_local_cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809d624-aa33-48a7-89dc-ffa73c79f0a9",
   "metadata": {},
   "source": [
    "## 2. Deployment to a SageMaker Endpoint using a SageMaker LMI Docker image and the HuggingFace Accelerate engine\n",
    "Start up of LLM inference containers can last longer than for smaller models mainly because of longer model downloading and loading times. Timeout values need to be increased accordingly from their default values. Each endpoint deployment takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a341cc-1fff-47a8-944c-b9953df0544d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ARTIFACTS_DOWNLOAD_TIMEOUT_IN_SECS = 9 * 60\n",
    "CONTAINER_STARTUP_TIMEOUT_IN_SECS = 9 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36cda14-a3d9-43c8-b270-20360e9f0052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONTAINER_STARTUP_CONFIGURATION = {\n",
    "    \"model_data_download_timeout\": MODEL_ARTIFACTS_DOWNLOAD_TIMEOUT_IN_SECS,\n",
    "    \"container_startup_health_check_timeout\": CONTAINER_STARTUP_TIMEOUT_IN_SECS,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68712ce8-9940-40b0-8dec-c5faac2df72a",
   "metadata": {},
   "source": [
    "### 2.1. Inference using the default HuggingFace Accelerate handler\n",
    "In this section, you deploy the `lmsys/vicuna-7b-v1.3` model to a SageMaker endpoint consisting of a single `ml.g5.2xlarge` instance. The inference engine used by the DJL Serving stack is HuggingFace Accelerate. Chosen precision is FP16 (native precision). and using the HuggingFace Accelerate handler as inference engine (referred as the `Python` engine in the [DJL Serving general settings](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html)).\n",
    "\n",
    "To each engine corresponds a dedicated `sagemaker.model.Model` class. In the present case, you will use the `sagemaker.djl_inference.HuggingFaceAccelerateModel` class. The model server configuration is generated by the `HuggingFaceAccelerateModel` class from the arguments we pass to its constructor and from an optional and already-existing `serving.properties` file.\n",
    "\n",
    "Since the HuggingFace Accelerate [default handler script](https://github.com/deepjavalibrary/djl-serving/blob/master/engines/python/setup/djl_python/huggingface.py) natively supports response streaming, we do not implement any custom server-side handler script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735dcd6-871c-49ff-b450-ea19a517d98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.djl_inference import HuggingFaceAccelerateModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32fafe-35f3-4966-8843-37b8d2c49705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SOURCE_DIR_ACCELERATE = Path(\"code-accelerate\")\n",
    "SOURCE_DIR_ACCELERATE.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed10c6-e78d-4720-9aed-9238dfc734b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code-accelerate/serving.properties\n",
    "option.enable_streaming=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bfc738-9a47-4be8-b7c8-5761e80686d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code-accelerate/requirements.txt\n",
    "protobuf<3.20\n",
    "transformers>=4.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b497f2-d15b-407a-8609-bd9a9a0c3bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_accelerate_model = HuggingFaceAccelerateModel(\n",
    "    djl_version=DJL_VERSION,\n",
    "    model_id=MODEL_ID,\n",
    "    source_dir=SOURCE_DIR_ACCELERATE.as_posix(),\n",
    "    role=SM_DEFAULT_EXECUTION_ROLE_ARN,\n",
    "    task=\"text-generation\",\n",
    "    # HF Accelerate configuration arguments\n",
    "    dtype=\"fp16\",\n",
    "    number_of_partitions=1,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    load_in_8bit=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19af85-3658-43c8-831a-ec662db9794f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_accelerate_predictor = hf_accelerate_model.deploy(\n",
    "    instance_type=INSTANCE_TYPE, initial_instance_count=1, **CONTAINER_STARTUP_CONFIGURATION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f2588-65d2-4a4c-8668-495d42034a93",
   "metadata": {
    "tags": []
   },
   "source": [
    "***Notices:***\n",
    "* Requests with response streaming currently do not support multiple input prompts\n",
    "* The `Predictor` object returned by the `deploy` method is currently not capable of invoking the endpoint it is tied to with response streaming. We therefore use the lower-level `boto3` client to invoke the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea90ff8-e55e-4498-b573-e3085c96780c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = hf_accelerate_predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541095f0-c114-4244-8c8c-abe19866f9c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "prompts = [\n",
    "    \"What is Amazon? Be concise.\"\n",
    "]\n",
    "request_content_type = \"application/json\"\n",
    "response_content_type = \"application/jsonlines\"\n",
    "\n",
    "request_body = {\"inputs\": [PROMPT_TEMPLATE.format(prompt=prompt) for prompt in prompts], \n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": 128,\n",
    "                    \"do_sample\": True,\n",
    "                    \"temperature\": 1.1,\n",
    "                    \"top_p\": 0.85,\n",
    "                },\n",
    "               }\n",
    "\n",
    "response = SAGEMAKER_RUNTIME_CLIENT.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name, \n",
    "    Body=json.dumps(request_body), \n",
    "    ContentType=request_content_type,\n",
    "    Accept=response_content_type,\n",
    ")\n",
    "\n",
    "event_stream = response['Body']\n",
    "scanner = StreamScanner()\n",
    "for event in event_stream:\n",
    "    scanner.write(event['PayloadPart']['Bytes'])\n",
    "    for line in scanner.readlines():\n",
    "        deserialized_line = json.loads(line)\n",
    "        print(deserialized_line.get(\"outputs\")[0], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184daae0-aa49-4b7d-ae4f-4e46df04126c",
   "metadata": {},
   "source": [
    "Now let's delete the endpoint to redeploy the model with a custom server-side handler script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8911718-072a-47f4-9ea4-8976c69b5d28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean-up\n",
    "hf_accelerate_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "hf_accelerate_model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a2a6e7-738e-4746-990d-54e5ca0aa6a4",
   "metadata": {},
   "source": [
    "### 2.2. Inference using a custom server-side handler script\n",
    "In this section, you will redeploy the same model but first, you will add a custom Python server-side handler script to the code artifacts that are to be deployed to the container (gathered in the `source_dir` / `SOURCE_DIR_ACCELERATE` directory).\n",
    "\n",
    "The custom handler script below allows to enable or disable streaming on a per-request basis. Default behavior is set using the `option.enable_streaming` field to `true` in the model server's configuration file `serving.properties`.\n",
    "\n",
    "The custom handler script allows to showcase the main differences when enabling streaming in the LMI container compared to sending the full generated sequences once:\n",
    "* We use a streamer object, `djl_python.streaming_utils.HFStreamer`, which implements the interface defined by [`transformers.TextStreamer`](https://huggingface.co/docs/transformers/v4.31.0/en/internal/generation_utils#transformers.TextStreamer). The streamer object uses the model's tokenizer to decode the generated token Ids before pushing them to the streamer's internal queue. The `transformers.generation.streamers.TextIteratorStreamer` streamer can be used as an alternative.\n",
    "* Instead of adding the result to the `djl_python.Output` object, we attach the streamer object using its `add_stream_content`.\n",
    "* Generation is executed in a background thread. The streamer object is passed to the [`generate` method](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/text_generation#transformers.GenerationMixin.generate) together with the `GenerationConfig`. The generation logic then uses the streamer to post tokens to the streamer's queue. On the other side, since the `Output` object has access to the streamer object, it is able to retrieve the tokens from its queue to dispatch them to the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea68997-72c3-4a5b-a558-2d0528e3f74e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code-accelerate/handler.py\n",
    "import os\n",
    "from threading import Thread\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from djl_python import Input, Output\n",
    "from djl_python.streaming_utils import HFStreamer\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer, GenerationConfig, TextGenerationPipeline\n",
    "from transformers.generation.streamers import BaseStreamer, TextIteratorStreamer\n",
    "\n",
    "def get_torch_dtype_from_str(dtype: str) -> torch.dtype:\n",
    "    if dtype == \"fp32\":\n",
    "        return torch.float32\n",
    "    if dtype == \"fp16\":\n",
    "        return torch.float16\n",
    "    if dtype == \"bf16\":\n",
    "        return torch.bfloat16\n",
    "    if dtype == \"int8\":\n",
    "        return torch.int8\n",
    "    if dtype is None:\n",
    "        return None\n",
    "    raise ValueError(f\"Data type cannot be parsed as valid Torch data type: {dtype}\")\n",
    "\n",
    "    \n",
    "def start_generation_thread(model: TextGenerationPipeline, streamer: BaseStreamer, input_sequences: List[str], generation_config: GenerationConfig) -> None:\n",
    "    def run_generation_with_streaming(model: TextGenerationPipeline, streamer: BaseStreamer, input_sequences: List[str], generation_config: GenerationConfig) -> None:\n",
    "        try:\n",
    "            model.generate(input_sequences, streamer=streamer, generation_config=generation_config)\n",
    "        except Exception as e:\n",
    "            streamer.put_text(str(e))\n",
    "        finally:\n",
    "            streamer.end()\n",
    "\n",
    "    thread = Thread(target=run_generation_with_streaming,\n",
    "                    args=[model, streamer, input_sequences, generation_config],\n",
    "                   )\n",
    "    thread.start()    \n",
    "\n",
    "\n",
    "class ConfigFactory:\n",
    "    \n",
    "    def __init__(self, properties: Dict[str, Any]) -> None:\n",
    "        self._properties = properties\n",
    "        self._dtype = get_torch_dtype_from_str(properties.get(\"dtype\", \"fp16\"))\n",
    "        \n",
    "    def build_model_loading_config(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"low_cpu_mem_usage\": (self._properties.get(\"low_cpu_mem_usage\", \"true\").lower() == \"true\"),\n",
    "            \"trust_remote_code\": (self._properties.get(\"trust_remote_code\", \"false\").lower() == \"true\"), \n",
    "            \"local_files_only\": (self._properties.get(\"local_files_only\", \"false\").lower() == \"true\"),\n",
    "            \"torch_dtype\": self._dtype,\n",
    "            \"revision\": self._properties.get(\"revision\", \"main\"),\n",
    "            \"device_map\": self._properties.get(\"device_map\", \"auto\"),\n",
    "        }\n",
    "    \n",
    "    def build_tokenizer_loading_config(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"trust_remote_code\": (self._properties.get(\"trust_remote_code\", \"false\").lower() == \"true\"), \n",
    "            \"revision\": self._properties.get(\"revision\", \"main\"),\n",
    "            \"legacy\": (self._properties.get(\"tokenizer_legacy_behavior\", \"false\").lower() == \"true\"),\n",
    "        }\n",
    "    \n",
    "    def build_tokenizer_encoding_config(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"padding\": True, \n",
    "            \"return_tensors\": \"pt\"\n",
    "        }\n",
    "    \n",
    "    def build_tokenizer_decoding_config(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"skip_special_tokens\": self._properties.get(\"skip_special_tokens\", True)\n",
    "        }\n",
    "\n",
    "\n",
    "class HuggingFaceAccelerateInferenceService:\n",
    "    def __init__(self) -> None:\n",
    "        self.model_location = None\n",
    "        self._config_factory = None\n",
    "        self._tokenizer = None\n",
    "        self._model = None\n",
    "        self.initialized = False\n",
    "        self.default_is_streaming_enabled = None\n",
    "        self._default_generation_parameters = {}\n",
    "        self.device = None\n",
    "    \n",
    "    def _load_tokenizer(self) -> PreTrainedTokenizer:\n",
    "        tokenizer_loading_config = self._config_factory.build_tokenizer_loading_config()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_location, **tokenizer_loading_config)\n",
    "        if not tokenizer.pad_token:\n",
    "            tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "            self._default_generation_parameters.update({\"pad_token_id\": tokenizer.pad_token_id})\n",
    "        return tokenizer\n",
    "    \n",
    "    def initialize(self, properties: Dict[str, str]) -> None:\n",
    "        self._config_factory = ConfigFactory(properties=properties) \n",
    "        # model_id can point to huggingface model_id or local directory.\n",
    "        # If option.model_id points to a s3 bucket, the DJL model server downloads it and set option.model_id to the local download directory.\n",
    "        # If option.model_id is not available, it is assumed model artifacts are in the option.model_dir (by default set to /opt/ml/model, which is also the cwd)\n",
    "        self.model_location = properties.get(\"model_id\") or properties.get(\"model_dir\")\n",
    "        self.default_is_streaming_enabled = (properties.get(\"enable_streaming\", \"true\").lower() == \"true\")\n",
    "        device_id = properties.get(\"device_id\", None)\n",
    "        self.device = f\"cuda:{device_id}\" if device_id is not None else \"cuda:0\"\n",
    "        \n",
    "        self._tokenizer = self._load_tokenizer()\n",
    "        model_loading_config = self._config_factory.build_model_loading_config()\n",
    "        self._model = AutoModelForCausalLM.from_pretrained(self.model_location, **model_loading_config).to(self.device)\n",
    "        self.initialized = True\n",
    "        \n",
    "    def handle_generation_request(self, inputs: Input) -> Output:\n",
    "        request_payload = inputs.get_as_json()\n",
    "        input_sequences = request_payload[\"inputs\"]\n",
    "        request_parameters = request_payload[\"parameters\"]\n",
    "        is_streaming_enabled = request_parameters.pop(\"stream_response\", self.default_is_streaming_enabled)\n",
    "        generation_parameters = self._default_generation_parameters.copy()\n",
    "        generation_parameters.update(request_parameters)\n",
    "        generation_config=GenerationConfig(**generation_parameters)\n",
    "        outputs = Output()\n",
    "        encoding_config = self._config_factory.build_tokenizer_encoding_config()\n",
    "        decoding_config = self._config_factory.build_tokenizer_decoding_config()\n",
    "        inputs = self._tokenizer(input_sequences, **encoding_config).input_ids.to(self.device)\n",
    "        if is_streaming_enabled:\n",
    "            assert len(input_sequences) == 1, \"Only one sequence can be processed at a time when stream_response=True\"\n",
    "            #streamer = HFStreamer(tokenizer=self._tokenizer, **decoding_config)\n",
    "            #streamer = TextIteratorStreamer(tokenizer=self._tokenizer, skip_prompt=True, **decoding_config)\n",
    "            streamer = TextIteratorStreamer(tokenizer=self._tokenizer, skip_prompt=True, **decoding_config)\n",
    "            start_generation_thread(model=self._model, streamer=streamer, input_sequences=inputs, generation_config=generation_config)\n",
    "            outputs.add_property(\"content-type\", \"application/jsonlines\")\n",
    "            outputs.add_stream_content(streamer)\n",
    "        else:\n",
    "            output_ids = self._model.generate(inputs, generation_config=generation_config)\n",
    "            output_sequences = self._tokenizer.batch_decode(output_ids, **decoding_config)\n",
    "            output_sequences = [output_seq[len(input_seq):] for input_seq, output_seq in zip(input_sequences, output_sequences)]\n",
    "            outputs.add_property(\"content-type\", \"application/json\")\n",
    "            outputs.add(output_sequences)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "_service = HuggingFaceAccelerateInferenceService()\n",
    "\n",
    "def handle(inputs: Input) -> Optional[Output]:\n",
    "    if not _service.initialized:\n",
    "        print(\"Initializing inference service\")\n",
    "        _service.initialize(properties=inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "\n",
    "    return _service.handle_generation_request(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86870393-235c-4faf-8c12-57eae29f7536",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_accelerate_model = HuggingFaceAccelerateModel(\n",
    "    djl_version=DJL_VERSION,\n",
    "    model_id=MODEL_ID,\n",
    "    source_dir=SOURCE_DIR_ACCELERATE.as_posix(),\n",
    "    entry_point=\"handler.py\",\n",
    "    role=SM_DEFAULT_EXECUTION_ROLE_ARN,\n",
    "    task=\"text-generation\",\n",
    "    # HF Accelerate configuration arguments\n",
    "    dtype=\"fp16\",\n",
    "    device_id=0,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    load_in_8bit=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb1c15-7fdd-4a6e-88f5-272f5bf9f31e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_accelerate_predictor = hf_accelerate_model.deploy(\n",
    "    instance_type=INSTANCE_TYPE, initial_instance_count=1, **CONTAINER_STARTUP_CONFIGURATION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2d8a9-8da0-4334-ae99-7ac52f879e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = hf_accelerate_predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c1af57-ed4f-4c06-a679-7a5b7e897600",
   "metadata": {},
   "source": [
    "The custom handler script allow to either stream the response tokens (default behavior), i.e. invoke the endpoint using `sagemaker:InvokeEndpointWithResponseStreaming` or to disable streaming at the request level, i.e. invoke the endpoint using `sagemaker:InvokeEndpoint`. Let's first invoke the endpoint with the streaming feature enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e875f9a-7509-4f0d-85fd-f6f3f7ac1108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "prompts = [\n",
    "    \"What is Amazon? Be concise.\"\n",
    "]\n",
    "request_content_type = \"application/json\"\n",
    "response_content_type = \"application/jsonlines\"\n",
    "\n",
    "request_body = {\"inputs\": [PROMPT_TEMPLATE.format(prompt=prompt) for prompt in prompts], \n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": 128,\n",
    "                    \"do_sample\": True,\n",
    "                    \"temperature\": 1.1,\n",
    "                    \"top_p\": 0.85,\n",
    "                },\n",
    "               }\n",
    "\n",
    "response = SAGEMAKER_RUNTIME_CLIENT.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name, \n",
    "    Body=json.dumps(request_body), \n",
    "    ContentType=request_content_type,\n",
    "    Accept=response_content_type,\n",
    ")\n",
    "\n",
    "event_stream = response['Body']\n",
    "scanner = StreamScanner()\n",
    "for event in event_stream:\n",
    "    scanner.write(event['PayloadPart']['Bytes'])\n",
    "    for line in scanner.readlines():\n",
    "        deserialized_line = json.loads(line)\n",
    "        print(deserialized_line.get(\"outputs\"), end='')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3793a6f3-795f-4608-a1f1-7ae404165c1f",
   "metadata": {},
   "source": [
    "Now let's add a `stream_response: False` entry to our request parameters to allow our endpoint to be invoked using the `sagemaker:InvokeEndpoint` API call and let's use the `Predictor` object returned by `Model.deploy` to perform this call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabca721-ddf9-48c2-b7bd-37b085f9ea1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prompts = [\n",
    "    \"What is Amazon? Be concise.\"\n",
    "]\n",
    "\n",
    "generation_config = {\n",
    "                    \"max_new_tokens\": 128,\n",
    "                    \"do_sample\": True,\n",
    "                    \"temperature\": 1.1,\n",
    "                    \"top_p\": 0.85,\n",
    "                    \"stream_response\": False,\n",
    "}\n",
    "\n",
    "hf_accelerate_predictor.predict(\n",
    "            data={\n",
    "                \"inputs\": [PROMPT_TEMPLATE.format(prompt=prompt) for prompt in prompts],\n",
    "                \"parameters\": generation_config,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2cdebe-7673-4a05-9a7d-7c5714ba1b1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean-up\n",
    "hf_accelerate_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "hf_accelerate_model.delete_model()\n",
    "shutil.rmtree(SOURCE_DIR_ACCELERATE.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e74f0-c489-4f2b-b468-f94caa2f1305",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Clean-up\n",
    "At this stage:\n",
    "* All your SageMaker Endpoint resources are supposed to be deleted, along with the SageMaker EndpointConfig and SageMaker Model resources they were associated with,\n",
    "* You have freed the disk space of your local host from the large model artifacts downloaded from the HuggingFace Hub.\n",
    "\n",
    "The only remaining cleanup task consist of removing the model artifacts from Amazon S3. This is what performs the next and last cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e601e69-6702-4514-b11e-48b9bd5ec55f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove HF model artifacts from S3\n",
    "hf_s3_objects = list_s3_objects(bucket=SM_ARTIFACT_BUCKET_NAME, key_prefix=HF_MODEL_KEY_PREFIX)\n",
    "hf_s3_objects_keys = [obj[\"Key\"] for obj in hf_s3_objects]\n",
    "delete_s3_objects(bucket=SM_ARTIFACT_BUCKET_NAME, keys=hf_s3_objects_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b719f-fc9e-4c58-898c-f7c08699ab6c",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/inference|generativeai|llm-workshop|lab6-llm-token-streaming|lab6-token-streaming-lmsys-vicuna-7b-lmi.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
